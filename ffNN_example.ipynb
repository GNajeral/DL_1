{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYuALZOG-AMq"
   },
   "source": [
    "## Assignment: Image recognition\n",
    "- Alumno 1: Victor Morcuende Castell\n",
    "- Alumno 2: Guillermo Najera Lavid\n",
    "- Alumno 3:\n",
    "\n",
    "The goals of the assignment are:\n",
    "* Develop proficiency in using Tensorflow/Keras for training Neural Nets (NNs).\n",
    "* Put into practice the acquired knowledge to optimize the parameters and architecture of a feedforward Neural Net (ffNN), in the context of an image recognition problem (28th Feb - 7th Mar).\n",
    "* Put into practice NNs specially conceived for analysing images. Design and optimize the parameters of a Convolutional Neural Net (CNN) to deal with previous task (7th Mar - 14th Mar).\n",
    "* Train popular architectures from scratch (e.g., GoogLeNet, VGG, ResNet, ...), and compare the results with the ones provided by their pre-trained versions using transfer learning (14th Mar - 23rd Mar)\n",
    "\n",
    "Follow the link below to download the classification data set  “xview_recognition”: [https://drive.upm.es/s/UeDNT7JEYFkCVPP](https://drive.upm.es/s/UeDNT7JEYFkCVPP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "logical_devices = tf.config.list_logical_devices('GPU')\n",
    "print(logical_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OYtqD3Oh-AMw"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import numpy as np\n",
    "\n",
    "class GenericObject:\n",
    "    \"\"\"\n",
    "    Generic object data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.id = uuid.uuid4()\n",
    "        self.bb = (-1, -1, -1, -1)\n",
    "        self.category= -1\n",
    "        self.score = -1\n",
    "\n",
    "class GenericImage:\n",
    "    \"\"\"\n",
    "    Generic image data.\n",
    "    \"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.tile = np.array([-1, -1, -1, -1])  # (pt_x, pt_y, pt_x+width, pt_y+height)\n",
    "        self.objects = list([])\n",
    "\n",
    "    def add_object(self, obj: GenericObject):\n",
    "        self.objects.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "I_GygShu-AMz"
   },
   "outputs": [],
   "source": [
    "categories = {0: 'CARGO_PLANE', 1: 'HELICOPTER', 2: 'SMALL_CAR', 3: 'BUS', 4: 'TRUCK', 5: 'MOTORBOAT', 6: 'FISHING_VESSEL', 7: 'DUMP_TRUCK', 8: 'EXCAVATOR', 9: 'BUILDING', 10: 'STORAGE_TANK', 11: 'SHIPPING_CONTAINER'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fRBA7ReQ-AM0"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def load_geoimage(filename):\n",
    "    warnings.filterwarnings('ignore', category=rasterio.errors.NotGeoreferencedWarning)\n",
    "    src_raster = rasterio.open(filename, 'r')\n",
    "    # RasterIO to OpenCV (see inconsistencies between libjpeg and libjpeg-turbo)\n",
    "    input_type = src_raster.profile['dtype']\n",
    "    input_channels = src_raster.count\n",
    "    img = np.zeros((src_raster.height, src_raster.width, src_raster.count), dtype=input_type)\n",
    "    for band in range(input_channels):\n",
    "        img[:, :, band] = src_raster.read(band+1)\n",
    "    return img\n",
    "\n",
    "def generator_images(objs, batch_size, do_shuffle=False):\n",
    "    while True:\n",
    "        if do_shuffle:\n",
    "            np.random.shuffle(objs)\n",
    "        groups = [objs[i:i+batch_size] for i in range(0, len(objs), batch_size)]\n",
    "        for group in groups:\n",
    "            images, labels = [], []\n",
    "            for (filename, obj) in group:\n",
    "                # Load image\n",
    "                images.append(load_geoimage(filename))\n",
    "                probabilities = np.zeros(len(categories))\n",
    "                probabilities[list(categories.values()).index(obj.category)] = 1\n",
    "                labels.append(probabilities)\n",
    "            images = np.array(images).astype(np.float32)\n",
    "            labels = np.array(labels).astype(np.float32)\n",
    "            yield images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HAanJ-V0-AM1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def draw_confusion_matrix(cm, categories):\n",
    "    # Draw confusion matrix\n",
    "    fig = plt.figure(figsize=[6.4*pow(len(categories), 0.5), 4.8*pow(len(categories), 0.5)])\n",
    "    ax = fig.add_subplot(111)\n",
    "    cm = cm.astype('float') / np.maximum(cm.sum(axis=1)[:, np.newaxis], np.finfo(np.float64).eps)\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.get_cmap('Blues'))\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]), xticklabels=list(categories.values()), yticklabels=list(categories.values()), ylabel='Annotation', xlabel='Prediction')\n",
    "    # Rotate the tick labels and set their alignment\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    # Loop over data dimensions and create text annotations\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], '.2f'), ha=\"center\", va=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\", fontsize=int(20-pow(len(categories), 0.5)))\n",
    "    fig.tight_layout()\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diNBB3qy-AM2"
   },
   "source": [
    "#### Training\n",
    "Design and train a ffNN to deal with the “xview_recognition” classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Orto292C-AM3"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load database\n",
    "json_file = \"C:/Users/guill/Downloads/xview_recognition/xview_recognition/xview_ann_train.json\"\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "ifs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4GjFLHs4-AM4",
    "outputId": "5581df22-d4e9-42ac-9f94-061fd8c7acd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CARGO_PLANE': 635, 'HELICOPTER': 70, 'SMALL_CAR': 11675, 'BUS': 4017, 'TRUCK': 5836, 'MOTORBOAT': 571, 'FISHING_VESSEL': 457, 'DUMP_TRUCK': 1108, 'EXCAVATOR': 741, 'BUILDING': 13011, 'STORAGE_TANK': 1249, 'SHIPPING_CONTAINER': 816}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "counts = dict.fromkeys(categories.values(), 0)\n",
    "anns = []\n",
    "for json_img, json_ann in zip(json_data['images'], json_data['annotations']):\n",
    "    image = GenericImage(\"C:/Users/guill/Downloads/xview_recognition/xview_recognition/\" + json_img['file_name'])\n",
    "    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n",
    "    obj = GenericObject()\n",
    "    obj.id = json_ann['id']\n",
    "    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n",
    "    obj.category = list(categories.values())[json_ann['category_id']-1]\n",
    "    # Resampling strategy to reduce training time\n",
    "    counts[obj.category] += 1\n",
    "    image.add_object(obj)\n",
    "    anns.append(image)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NriAECvS-AM6"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "anns_train, anns_valid = train_test_split(anns, test_size=0.1, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "BNkjbY2e-AM7",
    "outputId": "47bde031-306f-464e-8e22-cc70a7fb7c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 150528)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                9633856   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 12)                3084      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 13,633,996\n",
      "Trainable params: 13,624,396\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D, LeakyReLU\n",
    "from keras.regularizers import l1, l2\n",
    "\n",
    "# A possible option to balance the dataset: oversampling the minority classes\n",
    "# Number 42 is the seed, used to ensure that the same random numbers are generated each time the code is run\n",
    "'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "anns_train_resampled, anns_valid_resampled = sm.fit_resample(anns_train, anns_valid)\n",
    "'''\n",
    "\n",
    "# A possible option to balance the dataset: undersampling the majority classes\n",
    "'''\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "anns_train_resampled, anns_valid_resampled = rus.fit_resample(anns_train, anns_valid)\n",
    "'''\n",
    "\n",
    "print('Load model')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(224, 224, 3)))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1024))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(2048, kernel_regularizer=l1(0.001), bias_regularizer=l1(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(512, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(256, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model.add(Dense(len(categories), kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-aSlKtG6-AM7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Learning rate is changed to 0.001\n",
    "opt = Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.00, amsgrad=True, clipnorm=1.0, clipvalue=0.5)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GGAJEfpB-AM8"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TerminateOnNaN, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Callbacks\n",
    "model_checkpoint = ModelCheckpoint('model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau('val_accuracy', factor=0.1, patience=10, verbose=1)\n",
    "early_stop = EarlyStopping('val_accuracy', patience=40, verbose=1)\n",
    "terminate = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint, reduce_lr, early_stop, terminate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Yht-QqUH-AM8"
   },
   "outputs": [],
   "source": [
    "# Generate the list of objects from annotations\n",
    "objs_train = [(ann.filename, obj) for ann in anns_train for obj in ann.objects]\n",
    "objs_valid = [(ann.filename, obj) for ann in anns_valid for obj in ann.objects]\n",
    "# Generators\n",
    "batch_size = 16\n",
    "train_generator = generator_images(objs_train, batch_size, do_shuffle=True)\n",
    "valid_generator = generator_images(objs_valid, batch_size, do_shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrfpdECs-AM9",
    "outputId": "21d89b78-d94c-442e-9bc2-517654c0b614",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Epoch 1/70\n",
      "2261/2261 [==============================] - 323s 141ms/step - loss: 7.5135 - accuracy: 0.3537 - val_loss: 4.3164 - val_accuracy: 0.3882\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.38816, saving model to model.hdf5\n",
      "Epoch 2/70\n",
      "2261/2261 [==============================] - 288s 127ms/step - loss: 3.9552 - accuracy: 0.4040 - val_loss: 3.7877 - val_accuracy: 0.3936\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.38816 to 0.39363, saving model to model.hdf5\n",
      "Epoch 3/70\n",
      "2261/2261 [==============================] - 287s 127ms/step - loss: 3.6840 - accuracy: 0.4158 - val_loss: 3.7514 - val_accuracy: 0.4282\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.39363 to 0.42822, saving model to model.hdf5\n",
      "Epoch 4/70\n",
      "2261/2261 [==============================] - 282s 125ms/step - loss: 3.5647 - accuracy: 0.4227 - val_loss: 3.3477 - val_accuracy: 0.4245\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.42822\n",
      "Epoch 5/70\n",
      "2261/2261 [==============================] - 284s 126ms/step - loss: 3.4721 - accuracy: 0.4323 - val_loss: 3.4953 - val_accuracy: 0.4402\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.42822 to 0.44016, saving model to model.hdf5\n",
      "Epoch 6/70\n",
      "2261/2261 [==============================] - 288s 127ms/step - loss: 3.4182 - accuracy: 0.4393 - val_loss: 3.1822 - val_accuracy: 0.4715\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.44016 to 0.47151, saving model to model.hdf5\n",
      "Epoch 7/70\n",
      "2261/2261 [==============================] - 288s 127ms/step - loss: 3.3509 - accuracy: 0.4425 - val_loss: 3.3695 - val_accuracy: 0.4310\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.47151\n",
      "Epoch 8/70\n",
      "2261/2261 [==============================] - 288s 127ms/step - loss: 3.2886 - accuracy: 0.4477 - val_loss: 3.3581 - val_accuracy: 0.4583\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.47151\n",
      "Epoch 9/70\n",
      "2261/2261 [==============================] - 282s 125ms/step - loss: 3.2084 - accuracy: 0.4544 - val_loss: 3.2553 - val_accuracy: 0.4708\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.47151\n",
      "Epoch 10/70\n",
      "2261/2261 [==============================] - 281s 124ms/step - loss: 3.1734 - accuracy: 0.4587 - val_loss: 3.2902 - val_accuracy: 0.4058\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.47151\n",
      "Epoch 11/70\n",
      "2261/2261 [==============================] - 281s 124ms/step - loss: 3.1209 - accuracy: 0.4600 - val_loss: 3.0546 - val_accuracy: 0.4690\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.47151\n",
      "Epoch 12/70\n",
      "2261/2261 [==============================] - 290s 128ms/step - loss: 3.1000 - accuracy: 0.4648 - val_loss: 3.0738 - val_accuracy: 0.4713\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.47151\n",
      "Epoch 13/70\n",
      "2261/2261 [==============================] - 291s 129ms/step - loss: 3.0555 - accuracy: 0.4688 - val_loss: 2.9873 - val_accuracy: 0.4807\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.47151 to 0.48072, saving model to model.hdf5\n",
      "Epoch 14/70\n",
      "2261/2261 [==============================] - 290s 128ms/step - loss: 3.0538 - accuracy: 0.4665 - val_loss: 2.8130 - val_accuracy: 0.5076\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.48072 to 0.50759, saving model to model.hdf5\n",
      "Epoch 15/70\n",
      "2261/2261 [==============================] - 292s 129ms/step - loss: 3.0154 - accuracy: 0.4683 - val_loss: 2.8872 - val_accuracy: 0.5058\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.50759\n",
      "Epoch 16/70\n",
      "2261/2261 [==============================] - 294s 130ms/step - loss: 2.9657 - accuracy: 0.4739 - val_loss: 2.7801 - val_accuracy: 0.4904\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.50759\n",
      "Epoch 17/70\n",
      "2261/2261 [==============================] - 293s 129ms/step - loss: 2.9558 - accuracy: 0.4751 - val_loss: 2.9333 - val_accuracy: 0.4994\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.50759\n",
      "Epoch 18/70\n",
      "2261/2261 [==============================] - 294s 130ms/step - loss: 2.9366 - accuracy: 0.4767 - val_loss: 2.8407 - val_accuracy: 0.5066\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.50759\n",
      "Epoch 19/70\n",
      "2261/2261 [==============================] - 295s 130ms/step - loss: 2.9324 - accuracy: 0.4802 - val_loss: 2.9535 - val_accuracy: 0.5258\n",
      "\n",
      "Epoch 00019: val_accuracy improved from 0.50759 to 0.52575, saving model to model.hdf5\n",
      "Epoch 20/70\n",
      "2261/2261 [==============================] - 296s 131ms/step - loss: 2.9239 - accuracy: 0.4819 - val_loss: 2.9450 - val_accuracy: 0.4596\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.52575\n",
      "Epoch 21/70\n",
      "2261/2261 [==============================] - 295s 130ms/step - loss: 2.8894 - accuracy: 0.4810 - val_loss: 2.7572 - val_accuracy: 0.5163\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.52575\n",
      "Epoch 22/70\n",
      "2261/2261 [==============================] - 289s 128ms/step - loss: 2.8883 - accuracy: 0.4818 - val_loss: 2.7787 - val_accuracy: 0.5148\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.52575\n",
      "Epoch 23/70\n",
      "2261/2261 [==============================] - 288s 127ms/step - loss: 2.8584 - accuracy: 0.4796 - val_loss: 2.8735 - val_accuracy: 0.4678\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.52575\n",
      "Epoch 24/70\n",
      "2261/2261 [==============================] - 287s 127ms/step - loss: 2.8338 - accuracy: 0.4857 - val_loss: 2.7188 - val_accuracy: 0.5200\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.52575\n",
      "Epoch 25/70\n",
      "2261/2261 [==============================] - 285s 126ms/step - loss: 2.8086 - accuracy: 0.4881 - val_loss: 2.7749 - val_accuracy: 0.4471\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.52575\n",
      "Epoch 26/70\n",
      "2261/2261 [==============================] - 286s 127ms/step - loss: 2.7904 - accuracy: 0.4920 - val_loss: 2.6340 - val_accuracy: 0.5474\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.52575 to 0.54740, saving model to model.hdf5\n",
      "Epoch 27/70\n",
      "2261/2261 [==============================] - 287s 127ms/step - loss: 2.7580 - accuracy: 0.4944 - val_loss: 2.8554 - val_accuracy: 0.4971\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.54740\n",
      "Epoch 28/70\n",
      "2261/2261 [==============================] - 287s 127ms/step - loss: 2.7453 - accuracy: 0.4925 - val_loss: 2.6414 - val_accuracy: 0.5496\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.54740 to 0.54964, saving model to model.hdf5\n",
      "Epoch 29/70\n",
      "2261/2261 [==============================] - 287s 127ms/step - loss: 2.7405 - accuracy: 0.4896 - val_loss: 2.6898 - val_accuracy: 0.5367\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.54964\n",
      "Epoch 30/70\n",
      "2261/2261 [==============================] - 287s 127ms/step - loss: 2.7224 - accuracy: 0.4981 - val_loss: 2.6820 - val_accuracy: 0.5469\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.54964\n",
      "Epoch 31/70\n",
      "2261/2261 [==============================] - 288s 127ms/step - loss: 2.7017 - accuracy: 0.4973 - val_loss: 2.6856 - val_accuracy: 0.5389\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.54964\n",
      "Epoch 32/70\n",
      "2261/2261 [==============================] - 288s 127ms/step - loss: 2.6950 - accuracy: 0.4984 - val_loss: 2.6449 - val_accuracy: 0.5591\n",
      "\n",
      "Epoch 00032: val_accuracy improved from 0.54964 to 0.55909, saving model to model.hdf5\n",
      "Epoch 33/70\n",
      "2261/2261 [==============================] - 289s 128ms/step - loss: 2.6829 - accuracy: 0.5003 - val_loss: 2.6000 - val_accuracy: 0.5551\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.55909\n",
      "Epoch 34/70\n",
      "2261/2261 [==============================] - 289s 128ms/step - loss: 2.6607 - accuracy: 0.5049 - val_loss: 2.5709 - val_accuracy: 0.5432\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.55909\n",
      "Epoch 35/70\n",
      "2261/2261 [==============================] - 297s 131ms/step - loss: 2.6396 - accuracy: 0.5050 - val_loss: 2.6135 - val_accuracy: 0.5337\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.55909\n",
      "Epoch 36/70\n",
      " 382/2261 [====>.........................] - ETA: 3:42 - loss: 2.5957 - accuracy: 0.5069"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "print('Training model')\n",
    "epochs = 70\n",
    "train_steps = math.ceil(len(objs_train)/batch_size)\n",
    "valid_steps = math.ceil(len(objs_valid)/batch_size)\n",
    "h = model.fit(train_generator, steps_per_epoch=train_steps, validation_data=valid_generator, validation_steps=valid_steps, epochs=epochs, callbacks=callbacks, verbose=1)\n",
    "# Best validation model\n",
    "best_idx = int(np.argmax(h.history['val_accuracy']))\n",
    "best_value = np.max(h.history['val_accuracy'])\n",
    "print('Best validation model: epoch ' + str(best_idx+1), ' - val_accuracy ' + str(best_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IMMO_mT-AM9"
   },
   "source": [
    "#### Testing\n",
    "Try to improve the results provided in the Moodle competition wiki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sgh9KqIW-AM-",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load database\n",
    "json_file = \"C:/Users/guill/Downloads/xview_recognition/xview_recognition/xview_ann_test.json\"\n",
    "with open(json_file) as ifs:\n",
    "    json_data = json.load(ifs)\n",
    "ifs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJr_-xCt-AM-",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "anns = []\n",
    "for json_img, json_ann in zip(json_data['images'], json_data['annotations']):\n",
    "    image = GenericImage(\"C:/Users/guill/Downloads/xview_recognition/xview_recognition/\" + json_img['file_name'])\n",
    "    image.tile = np.array([0, 0, json_img['width'], json_img['height']])\n",
    "    obj = GenericObject()\n",
    "    obj.id = json_ann['id']\n",
    "    obj.bb = (int(json_ann['bbox'][0]), int(json_ann['bbox'][1]), int(json_ann['bbox'][2]), int(json_ann['bbox'][3]))\n",
    "    obj.category = list(categories.values())[json_ann['category_id']-1]\n",
    "    image.add_object(obj)\n",
    "    anns.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TGs2zqfv-AM_",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# model.load_weights('model.hdf5', by_name=True)\n",
    "y_true, y_pred = [], []\n",
    "for ann in anns:\n",
    "    # Load image\n",
    "    image = load_geoimage(ann.filename)\n",
    "    for obj_pred in ann.objects:\n",
    "        # Generate prediction\n",
    "        warped_image = np.expand_dims(image, 0)\n",
    "        predictions = model.predict(warped_image)\n",
    "        # Save prediction\n",
    "        pred_category = list(categories.values())[np.argmax(predictions)]\n",
    "        pred_score = np.max(predictions)\n",
    "        y_true.append(obj_pred.category)\n",
    "        y_pred.append(pred_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqYKVsEp-AM_",
    "outputId": "d256367d-744e-487d-f44c-cd106dad3484",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(categories.values()))\n",
    "draw_confusion_matrix(cm, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jD1zLfCd-ANA",
    "outputId": "3a3154f8-c349-4a58-a129-d7aa04a588cb",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute the accuracy\n",
    "correct_samples_class = np.diag(cm).astype(float)\n",
    "total_samples_class = np.sum(cm, axis=1).astype(float)\n",
    "total_predicts_class = np.sum(cm, axis=0).astype(float)\n",
    "print('Mean Accuracy: %.3f%%' % (np.sum(correct_samples_class) / np.sum(total_samples_class) * 100))\n",
    "acc = correct_samples_class / np.maximum(total_samples_class, np.finfo(np.float64).eps)\n",
    "print('Mean Recall: %.3f%%' % (acc.mean() * 100))\n",
    "acc = correct_samples_class / np.maximum(total_predicts_class, np.finfo(np.float64).eps)\n",
    "print('Mean Precision: %.3f%%' % (acc.mean() * 100))\n",
    "for idx in range(len(categories)):\n",
    "    # True/False Positives (TP/FP) refer to the number of predicted positives that were correct/incorrect.\n",
    "    # True/False Negatives (TN/FN) refer to the number of predicted negatives that were correct/incorrect.\n",
    "    tp = cm[idx, idx]\n",
    "    fp = sum(cm[:, idx]) - tp\n",
    "    fn = sum(cm[idx, :]) - tp\n",
    "    tn = sum(np.delete(sum(cm) - cm[idx, :], idx))\n",
    "    # True Positive Rate: proportion of real positive cases that were correctly predicted as positive.\n",
    "    recall = tp / np.maximum(tp+fn, np.finfo(np.float64).eps)\n",
    "    # Precision: proportion of predicted positive cases that were truly real positives.\n",
    "    precision = tp / np.maximum(tp+fp, np.finfo(np.float64).eps)\n",
    "    # True Negative Rate: proportion of real negative cases that were correctly predicted as negative.\n",
    "    specificity = tn / np.maximum(tn+fp, np.finfo(np.float64).eps)\n",
    "    # Dice coefficient refers to two times the intersection of two sets divided by the sum of their areas.\n",
    "    # Dice = 2 |A∩B| / (|A|+|B|) = 2 TP / (2 TP + FP + FN)\n",
    "    f1_score = 2 * ((precision * recall) / np.maximum(precision+recall, np.finfo(np.float64).eps))\n",
    "    print('> %s: Recall: %.3f%% Precision: %.3f%% Specificity: %.3f%% Dice: %.3f%%' % (list(categories.values())[idx], recall*100, precision*100, specificity*100, f1_score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOPI5zc8-ANB"
   },
   "source": [
    "#### Report\n",
    "\n",
    "You must prepare a report (PDF) describing:\n",
    "* The problems and data sets (briefly).\n",
    "* The process that you have followed to reach your solution for the “xview_recognition” benchmark, including your intermediate results. You must discuss and compare these results properly.\n",
    "* Final network architectures, including optimization algorithms, regularization methods (dropout, data augmentation, etc.), number of layers/parameters, and performance obtained with your model on the train/valid/test data sets, including the plots of the evolution of losses and accuracy.\n",
    "* It would also be very valuable your feedback on the use of “Cesvima” or “Google Colab\" services.\n",
    "\n",
    "In the submission via Moodle, attach your Python (.py) or Jupyter Notebook (.ipynb) source file, including in the report all results of computations attached to the code that generated them.\n",
    "\n",
    "The assignment must be done in groups of 3 students. Each team must submit one submission before Tuesday, April 18th, 2023, 23:55h."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNdH4hNj-ANB",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 (main, Jan 11 2023, 16:16:36) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "894dd180878d290e447042d97679c5a69d898b348486969cf3e91bcd729b88e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
